{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa008ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting onnxruntime-openvino\n",
      "  Downloading onnxruntime_openvino-1.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/mra/Desktop/machine-learning-practice/venv/lib/python3.12/site-packages (from onnx) (2.1.3)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /home/mra/Desktop/machine-learning-practice/venv/lib/python3.12/site-packages (from onnx) (5.29.5)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /home/mra/Desktop/machine-learning-practice/venv/lib/python3.12/site-packages (from onnx) (4.14.0)\n",
      "Collecting coloredlogs (from onnxruntime-openvino)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /home/mra/Desktop/machine-learning-practice/venv/lib/python3.12/site-packages (from onnxruntime-openvino) (25.2.10)\n",
      "Requirement already satisfied: packaging in /home/mra/Desktop/machine-learning-practice/venv/lib/python3.12/site-packages (from onnxruntime-openvino) (25.0)\n",
      "Collecting sympy (from onnxruntime-openvino)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-openvino)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime-openvino)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading onnx-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime_openvino-1.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (63.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, onnx, humanfriendly, coloredlogs, onnxruntime-openvino\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 mpmath-1.3.0 onnx-1.18.0 onnxruntime-openvino-1.22.0 sympy-1.14.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install ultralytics\n",
    "# !pip install deep_sort_realtime\n",
    "# !pip install openvino openvino-dev\n",
    "!pip install onnx onnxruntime-openvino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a79bd707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/5 /home/mra/Desktop/fyp_prototype/Data/sample_frames/frame_0001.jpg: 2048x3616 702 persons, 49551.0ms\n",
      "image 2/5 /home/mra/Desktop/fyp_prototype/Data/sample_frames/frame_0002.jpg: 2048x3616 555 persons, 49695.0ms\n",
      "image 3/5 /home/mra/Desktop/fyp_prototype/Data/sample_frames/frame_0003.jpg: 2048x3616 379 persons, 50167.9ms\n",
      "image 4/5 /home/mra/Desktop/fyp_prototype/Data/sample_frames/frame_0004.jpg: 2048x3616 488 persons, 50683.6ms\n",
      "image 5/5 /home/mra/Desktop/fyp_prototype/Data/sample_frames/frame_0005.jpg: 2048x3616 493 persons, 50172.8ms\n",
      "Speed: 74.2ms preprocess, 50054.1ms inference, 20.4ms postprocess per image at shape (1, 3, 2048, 3616)\n",
      "Done! Annotated frames with headâ€‘centers saved to: runs/detect/head_centers\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# STEP 0: Ensure dependencies are installed:\n",
    "#    pip install ultralytics opencv-python\n",
    "\n",
    "# STEP 1: Load the YOLOv8x PyTorch model\n",
    "model = YOLO('yolov8x.pt')\n",
    "\n",
    "# STEP 2: Run inference on your frames folder\n",
    "#    returns a Python list of Results, one per image\n",
    "results = model.predict(\n",
    "    source='Data/sample_frames/',\n",
    "    conf=0.01,         # catch distant people\n",
    "    classes=[0],       # only 'person'\n",
    "    imgsz=3616,        # resize input to 1280Ã—1280\n",
    "    max_det=1500,                   # Increase max detections (default=300)\n",
    "    agnostic_nms=True,  \n",
    "    device='cpu',      # or 'cuda' if available\n",
    ")\n",
    "\n",
    "# STEP 3: Prepare output folder\n",
    "output_dir = \"runs/detect/head_centers\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# STEP 4: Loop through results, draw headâ€‘center, and save\n",
    "for idx, res in enumerate(results):\n",
    "    # res.orig_img is the HÃ—WÃ—3 RGB array\n",
    "    img = res.orig_img.copy()\n",
    "    # convert to BGR for OpenCV\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Iterate all detected person boxes\n",
    "    for box in res.boxes:\n",
    "        # get integer coords\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        # head center = mid x of box, top y of box\n",
    "        head_x = int((x1 + x2) / 2)\n",
    "        head_y = y1\n",
    "\n",
    "        # draw a solid red dot (radius=8 px) at the head center\n",
    "        cv2.circle(img_bgr, (head_x, head_y), radius=8, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    # save annotated frame\n",
    "    frame_name = f\"frame_{idx:04d}.jpg\"\n",
    "    cv2.imwrite(os.path.join(output_dir, frame_name), img_bgr)\n",
    "\n",
    "print(f\"Done! Annotated frames with headâ€‘centers saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.161 ğŸš€ Python-3.12.3 torch-2.7.1+cu126 CPU (12th Gen Intel Core(TM) i5-1235U)\n",
      "YOLOv8x summary (fused): 112 layers, 68,200,608 parameters, 0 gradients, 257.8 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8x.pt' with input shape (1, 3, 3616, 3616) BCHW and output shape(s) (1, 84, 268149) (130.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2024.6.0-17404-4c0f47d2335-releases/2024/6...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success âœ… 469.8s, saved as 'yolov8x_openvino_model/' (263.6 MB)\n",
      "\n",
      "Export complete (653.9s)\n",
      "Results saved to \u001b[1m/home/mra/Desktop/fyp_prototype\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8x_openvino_model imgsz=3616  \n",
      "Validate:        yolo val task=detect model=yolov8x_openvino_model imgsz=3616 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov8x_openvino_model'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8x.pt')\n",
    "model.export(format='openvino', imgsz=3616)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2be3950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolov8x_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "0: 3616x3616 3 persons, 77943.5ms\n",
      "Speed: 242.6ms preprocess, 77943.5ms inference, 28.5ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 1 person, 8694.1ms\n",
      "Speed: 207.7ms preprocess, 8694.1ms inference, 9.1ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 10724.6ms\n",
      "Speed: 134.2ms preprocess, 10724.6ms inference, 12.2ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 10904.5ms\n",
      "Speed: 145.2ms preprocess, 10904.5ms inference, 10.4ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 5 persons, 11586.6ms\n",
      "Speed: 209.5ms preprocess, 11586.6ms inference, 12.0ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 2 persons, 11175.4ms\n",
      "Speed: 138.9ms preprocess, 11175.4ms inference, 10.8ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 5 persons, 11239.2ms\n",
      "Speed: 126.0ms preprocess, 11239.2ms inference, 11.1ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 11 persons, 11387.6ms\n",
      "Speed: 129.4ms preprocess, 11387.6ms inference, 11.3ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 24 persons, 10851.7ms\n",
      "Speed: 132.1ms preprocess, 10851.7ms inference, 11.3ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 9 persons, 11340.2ms\n",
      "Speed: 131.4ms preprocess, 11340.2ms inference, 11.9ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 9 persons, 10965.9ms\n",
      "Speed: 130.3ms preprocess, 10965.9ms inference, 9.2ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 19 persons, 11236.5ms\n",
      "Speed: 133.7ms preprocess, 11236.5ms inference, 12.9ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 1 person, 11233.9ms\n",
      "Speed: 126.7ms preprocess, 11233.9ms inference, 11.7ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 11118.4ms\n",
      "Speed: 125.2ms preprocess, 11118.4ms inference, 10.8ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 11453.6ms\n",
      "Speed: 133.7ms preprocess, 11453.6ms inference, 8.5ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 2 persons, 11165.5ms\n",
      "Speed: 140.0ms preprocess, 11165.5ms inference, 13.1ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 3 persons, 11083.2ms\n",
      "Speed: 124.1ms preprocess, 11083.2ms inference, 9.4ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 1 person, 11400.5ms\n",
      "Speed: 125.1ms preprocess, 11400.5ms inference, 9.4ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 3 persons, 10924.5ms\n",
      "Speed: 138.9ms preprocess, 10924.5ms inference, 10.7ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 16 persons, 11323.0ms\n",
      "Speed: 128.6ms preprocess, 11323.0ms inference, 11.1ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 26 persons, 11189.0ms\n",
      "Speed: 123.8ms preprocess, 11189.0ms inference, 13.2ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 19 persons, 10927.6ms\n",
      "Speed: 130.9ms preprocess, 10927.6ms inference, 9.3ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 13 persons, 11539.5ms\n",
      "Speed: 130.4ms preprocess, 11539.5ms inference, 10.0ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 30 persons, 11168.7ms\n",
      "Speed: 124.6ms preprocess, 11168.7ms inference, 12.1ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 2 persons, 11048.4ms\n",
      "Speed: 129.8ms preprocess, 11048.4ms inference, 15.4ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 10810.8ms\n",
      "Speed: 140.9ms preprocess, 10810.8ms inference, 12.0ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 1 person, 11050.7ms\n",
      "Speed: 153.4ms preprocess, 11050.7ms inference, 10.6ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 8 persons, 11166.0ms\n",
      "Speed: 123.8ms preprocess, 11166.0ms inference, 8.4ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 5 persons, 11160.4ms\n",
      "Speed: 127.5ms preprocess, 11160.4ms inference, 8.6ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 11141.7ms\n",
      "Speed: 131.5ms preprocess, 11141.7ms inference, 10.3ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 5 persons, 11128.3ms\n",
      "Speed: 131.5ms preprocess, 11128.3ms inference, 9.8ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 29 persons, 11174.4ms\n",
      "Speed: 123.5ms preprocess, 11174.4ms inference, 8.4ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 39 persons, 11193.9ms\n",
      "Speed: 118.7ms preprocess, 11193.9ms inference, 10.7ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 27 persons, 11027.9ms\n",
      "Speed: 124.1ms preprocess, 11027.9ms inference, 8.6ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 22 persons, 11147.2ms\n",
      "Speed: 124.6ms preprocess, 11147.2ms inference, 11.8ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 37 persons, 11110.6ms\n",
      "Speed: 124.1ms preprocess, 11110.6ms inference, 9.8ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 5 persons, 11109.6ms\n",
      "Speed: 150.4ms preprocess, 11109.6ms inference, 11.3ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 1 person, 11148.1ms\n",
      "Speed: 127.2ms preprocess, 11148.1ms inference, 8.3ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 10968.5ms\n",
      "Speed: 121.5ms preprocess, 10968.5ms inference, 8.7ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 2 persons, 11109.0ms\n",
      "Speed: 123.7ms preprocess, 11109.0ms inference, 9.2ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 11 persons, 11068.0ms\n",
      "Speed: 123.0ms preprocess, 11068.0ms inference, 10.0ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 1 person, 11126.7ms\n",
      "Speed: 123.8ms preprocess, 11126.7ms inference, 8.6ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 1 person, 11118.3ms\n",
      "Speed: 145.4ms preprocess, 11118.3ms inference, 9.4ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 18 persons, 10974.0ms\n",
      "Speed: 123.0ms preprocess, 10974.0ms inference, 9.9ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 31 persons, 11140.2ms\n",
      "Speed: 128.1ms preprocess, 11140.2ms inference, 9.7ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 25 persons, 11047.2ms\n",
      "Speed: 123.4ms preprocess, 11047.2ms inference, 8.8ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 17 persons, 10879.3ms\n",
      "Speed: 125.7ms preprocess, 10879.3ms inference, 9.7ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 19 persons, 11243.0ms\n",
      "Speed: 146.3ms preprocess, 11243.0ms inference, 11.0ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 2 persons, 10999.1ms\n",
      "Speed: 123.5ms preprocess, 10999.1ms inference, 8.8ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 11124.2ms\n",
      "Speed: 126.2ms preprocess, 11124.2ms inference, 10.9ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 11143.2ms\n",
      "Speed: 130.1ms preprocess, 11143.2ms inference, 8.5ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 (no detections), 10944.7ms\n",
      "Speed: 130.3ms preprocess, 10944.7ms inference, 8.5ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 8 persons, 11285.6ms\n",
      "Speed: 125.1ms preprocess, 11285.6ms inference, 9.4ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 4 persons, 11427.5ms\n",
      "Speed: 128.9ms preprocess, 11427.5ms inference, 8.8ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 3 persons, 11527.3ms\n",
      "Speed: 122.8ms preprocess, 11527.3ms inference, 9.2ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 19 persons, 10902.0ms\n",
      "Speed: 141.9ms preprocess, 10902.0ms inference, 9.0ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 20 persons, 11040.6ms\n",
      "Speed: 121.9ms preprocess, 11040.6ms inference, 9.9ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 21 persons, 10751.7ms\n",
      "Speed: 125.1ms preprocess, 10751.7ms inference, 8.9ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 12 persons, 10984.8ms\n",
      "Speed: 120.8ms preprocess, 10984.8ms inference, 9.0ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "\n",
      "0: 3616x3616 30 persons, 10978.2ms\n",
      "Speed: 139.3ms preprocess, 10978.2ms inference, 9.5ms postprocess per image at shape (1, 3, 3616, 3616)\n",
      "Done â€“ check runs/detect/head_centers_tiles\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "model = YOLO('yolov8x_openvino_model/')\n",
    "output_dir = \"runs/detect/head_centers_tiles\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "rows, cols = 4, 4\n",
    "keep_rows = range(1, 4)   # only rowâ€‘indices 1, 2, 3 (zeroâ€‘based) â†’ 12 tiles\n",
    "\n",
    "for idx, frame_path in enumerate(sorted(os.listdir('Data/sample_frames/'))):\n",
    "    img = cv2.imread(os.path.join('Data/sample_frames/', frame_path))\n",
    "    H, W = img.shape[:2]\n",
    "    tile_h, tile_w = H // rows, W // cols\n",
    "\n",
    "    all_boxes = []  # to collect detections across tiles\n",
    "\n",
    "    # 1) Loop over grid\n",
    "    for i in range(rows):\n",
    "        if i not in keep_rows:\n",
    "            continue\n",
    "        for j in range(cols):\n",
    "            # define tile origin\n",
    "            y0, x0 = i * tile_h, j * tile_w\n",
    "            # crop tile\n",
    "            tile = img[y0:y0 + tile_h, x0:x0 + tile_w]\n",
    "            # resize to match OpenVINO export\n",
    "            tile_resized = cv2.resize(tile, (3616, 3616))\n",
    "\n",
    "            # inference\n",
    "            res = model.predict(\n",
    "                source=tile_resized,\n",
    "                conf=0.01,\n",
    "                classes=[0],\n",
    "                device='cpu'\n",
    "            )\n",
    "\n",
    "            # rebase boxes\n",
    "            for box in res[0].boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                # scale back to original tile coords\n",
    "                x1 = x1 * (tile_w / 3616) + x0\n",
    "                x2 = x2 * (tile_w / 3616) + x0\n",
    "                y1 = y1 * (tile_h / 3616) + y0\n",
    "                y2 = y2 * (tile_h / 3616) + y0\n",
    "                all_boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "    # 2) Draw head centers on the full image\n",
    "    for (x1, y1, x2, y2) in all_boxes:\n",
    "        cx = int((x1 + x2) / 2)\n",
    "        head_y = int(y1)\n",
    "        cv2.circle(img, (cx, head_y), radius=8, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    # 3) Save result\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"frame_{idx:04d}.jpg\"), img)\n",
    "\n",
    "print(\"Done â€“ check\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f4633",
   "metadata": {},
   "source": [
    "# YOLO 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f602bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov12m.pt')\n",
    "model.export(format='openvino', imgsz=1920)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
